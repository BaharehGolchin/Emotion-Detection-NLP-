{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup File for Keras Models\n",
    "Use `%run Setup.ipynb` in another notebook to perform all these tasks automatically.\n",
    "\n",
    "Parameters that can be re-configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded Parameters:\n",
      " 40000 34 0.2 200 \n",
      " dataset/glove/glove.twitter.27B.200d.txt\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 40000 # max no. of words for tokenizer \n",
    "MAX_SEQUENCE_LENGTH = 30 # max length of text (words) including padding \\ 30 token aval ra dar nazar begir baghie ro dur beriz\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 200 # embedding dimensions for word vectors (word2vec/GloVe)\n",
    "GLOVE_DIR = \"dataset/glove/glove.twitter.27B.\"+str(200)+\"d.txt\"\n",
    "print(\"[i] Loaded Parameters:\\n\",\n",
    "      MAX_NB_WORDS,MAX_SEQUENCE_LENGTH+4,\n",
    "      VALIDATION_SPLIT,EMBEDDING_DIM,\"\\n\",\n",
    "      GLOVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Importing Modules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "print(\"[i] Importing Modules...\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, sys, os, csv, keras, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Using Keras version 2.2.2\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers, initializers, optimizers, callbacks\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.utils import plot_model\n",
    "print(\"[+] Using Keras version\",keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Finished Importing Modules\n"
     ]
    }
   ],
   "source": [
    "print(\"[+] Finished Importing Modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Reading from csv file...Done!\n"
     ]
    }
   ],
   "source": [
    "texts, labels = [], []\n",
    "print(\"[i] Reading from csv file...\", end=\"\")\n",
    "with open('data.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        texts.append(row[0])\n",
    "        labels.append(row[1])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text to word tokens (numbers that refer to the words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\\ntokenizer.fit_on_texts(texts)\\nwith open(\\'tokenizer.pickle\\', \\'wb\\') as handle:\\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\\nprint(\"[i] Saved word tokenizer to file: tokenizer.pickle\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"[i] Saved word tokenizer to file: tokenizer.pickle\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert tweets to sequences of word tokens with zero padding at the front and back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Found 34359 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('[i] Found %s unique tokens.' % len(word_index))\n",
    "#data_int = pad_sequences(sequences, padding='pre', maxlen=(MAX_SEQUENCE_LENGTH-5))\n",
    "data_int = pad_sequences(sequences, padding='pre', maxlen=(MAX_SEQUENCE_LENGTH-4))\n",
    "data = pad_sequences(data_int, padding='post', maxlen=(MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Shape of data tensor: (47288, 30)\n",
      "[+] Shape of label tensor: (47288, 5)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels)) # convert to one-hot encoding vectors\n",
    "print('[+] Shape of data tensor:', data.shape)\n",
    "print('[+] Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Number of entries in each category:\n",
      "[+] Training:\n",
      " [ 7705. 13044. 12716.  3476.   890.]\n",
      "[+] Validation:\n",
      " [1938. 3253. 3222.  825.  219.]\n"
     ]
    }
   ],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('[i] Number of entries in each category:')\n",
    "print(\"[+] Training:\\n\",y_train.sum(axis=0))\n",
    "print(\"[+] Validation:\\n\",y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Embedding layer\n",
    "\n",
    "Compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings.\n",
    "\n",
    "We use pre-trained [GloVe](https://nlp.stanford.edu/projects/glove/) vectors from Stanford NLP. For new words, a \"randomised vector\" will be created.\n",
    "می خواهد دیکشنری از کلمه به بردار بسازد که بعدا از این دیکشنری برای ساخت ماتریس استفاده می کند \n",
    "در واقع می خواهد محتوای فایل متنی گلاو را خط به خط بخواند و برای هر خطی  کلمه ی مربوطه کلید عنصر در دیکشنری و بردار آن کلمه ولیو آن کلمه بشود ***********\n",
    "بعد از ساخت دیکشنری می خواهیم ماتریس را بسازیم که سطرهای ماتریس کلمات و ستون ها به تعداد ابعاد، بردارهای کلمه هستند که به این ماتریس \n",
    "embedding matrix \n",
    "می گویند، دلیل ساخت این ماتریس این است که وزن لایه ی \n",
    "embedding \n",
    "را نشان می دهد و این ماتریس را درون لایه لود می کنیم و بعد فریز می کنیم تا \n",
    "train\n",
    "نشود\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loading GloVe from: dataset/glove/glove.twitter.27B.200d.txt ...Done.\n",
      "[+] Proceeding with Embedding Matrix...[i] Completed!\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {} #Dictionary kalame ---> bordar \n",
    "f = open(GLOVE_DIR, encoding=\"utf8\") \n",
    "print(\"[i] Loading GloVe from:\",GLOVE_DIR,\"...\",end=\"\")\n",
    "for line in f:              #iterate for each line \n",
    "    values = line.split()   #reshte haye yek matn ra ba space joda mikonad\n",
    "    word = values[0]        #avalin reshte kalame ast!\n",
    "    coefs = np.asarray(values[1:], dtype= 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(\"Done.\\n[+] Proceeding with Embedding Matrix...\", end=\"\")\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))#matrix khali misaze\n",
    "for word, i in word_index.items():  #iteration ruye kalame va index ha , ye dune ye dune kalamat ra begir\n",
    "    embedding_vector = embeddings_index.get(word)# vector haye kalamat ro estekhraj kon\n",
    "    #embeddings_index = dictionary word--->bordar\n",
    "    #word_index = dictionary word ---> index\n",
    "    if embedding_vector is not None: #agar kalame dar glove vojud dashte bashad bordaresh ra bar migardanad agar nabashad none bar migardanad.\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector #agar none nist, bordar ra dar satre i matrix gharar midahad.\n",
    "print(\"[i] Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Finished running setup.\n"
     ]
    }
   ],
   "source": [
    "print(\"[i] Finished running setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
